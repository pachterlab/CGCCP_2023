{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pachterlab/GCCP_2021/blob/main/test_colab_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LyHDS0r64Po"
      },
      "source": [
        "Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "nEiSpc7y8DWk",
        "outputId": "399be431-0339-4f13-cd7f-b73029327791"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAANT0lEQVR4nO3cYYjkd33H8ffHO1NpjKb0VpC706T00njYQtIlTRFqirZc8uDugUXuIFgleGAbKVWEFEuU+MiGWhCu1ZOKVdAYfSALntwDjQTEC7chNXgXItvTeheFrDHNk6Ax7bcPZtKdrneZf3Zndy/7fb/gYP7/+e3Mlx97752d2ZlUFZKk7e8VWz2AJGlzGHxJasLgS1ITBl+SmjD4ktSEwZekJqYGP8lnkzyZ5PuXuD5JPplkKcmjSW6c/ZiSpPUa8gj/c8CBF7n+VmDf+N9R4F/WP5YkadamBr+qHgR+/iJLDgGfr5FTwNVJXj+rASVJs7FzBrexGzg/cXxhfO6nqxcmOcrotwCuvPLKP7z++utncPeS1MfDDz/8s6qaW8vXziL4g1XVceA4wPz8fC0uLm7m3UvSy16S/1zr187ir3SeAPZOHO8Zn5MkXUZmEfwF4F3jv9a5GXimqn7t6RxJ0taa+pROki8BtwC7klwAPgK8EqCqPgWcAG4DloBngfds1LCSpLWbGvyqOjLl+gL+emYTSZI2hO+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqYlBwU9yIMnjSZaS3HWR69+Q5IEkjyR5NMltsx9VkrQeU4OfZAdwDLgV2A8cSbJ/1bK/B+6vqhuAw8A/z3pQSdL6DHmEfxOwVFXnquo54D7g0Ko1BbxmfPm1wE9mN6IkaRaGBH83cH7i+ML43KSPArcnuQCcAN5/sRtKcjTJYpLF5eXlNYwrSVqrWb1oewT4XFXtAW4DvpDk1267qo5X1XxVzc/Nzc3oriVJQwwJ/hPA3onjPeNzk+4A7geoqu8CrwJ2zWJASdJsDAn+aWBfkmuTXMHoRdmFVWt+DLwNIMmbGAXf52wk6TIyNfhV9TxwJ3ASeIzRX+OcSXJPkoPjZR8E3pvke8CXgHdXVW3U0JKkl27nkEVVdYLRi7GT5+6euHwWeMtsR5MkzZLvtJWkJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJHk8yVKSuy6x5p1JziY5k+SLsx1TkrReO6ctSLIDOAb8GXABOJ1koarOTqzZB/wd8JaqejrJ6zZqYEnS2gx5hH8TsFRV56rqOeA+4NCqNe8FjlXV0wBV9eRsx5QkrdeQ4O8Gzk8cXxifm3QdcF2S7yQ5leTAxW4oydEki0kWl5eX1zaxJGlNZvWi7U5gH3ALcAT4TJKrVy+qquNVNV9V83NzczO6a0nSEEOC/wSwd+J4z/jcpAvAQlX9qqp+CPyA0Q8ASdJlYkjwTwP7klyb5ArgMLCwas3XGD26J8kuRk/xnJvdmJKk9Zoa/Kp6HrgTOAk8BtxfVWeS3JPk4HjZSeCpJGeBB4APVdVTGzW0JOmlS1VtyR3Pz8/X4uLilty3JL1cJXm4qubX8rW+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmBgU/yYEkjydZSnLXi6x7R5JKMj+7ESVJszA1+El2AMeAW4H9wJEk+y+y7irgb4CHZj2kJGn9hjzCvwlYqqpzVfUccB9w6CLrPgZ8HPjFDOeTJM3IkODvBs5PHF8Yn/s/SW4E9lbV11/shpIcTbKYZHF5efklDytJWrt1v2ib5BXAJ4APTltbVcerar6q5ufm5tZ715Kkl2BI8J8A9k4c7xmfe8FVwJuBbyf5EXAzsOALt5J0eRkS/NPAviTXJrkCOAwsvHBlVT1TVbuq6pqqugY4BRysqsUNmViStCZTg19VzwN3AieBx4D7q+pMknuSHNzoASVJs7FzyKKqOgGcWHXu7kusvWX9Y0mSZs132kpSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmhgU/CQHkjyeZCnJXRe5/gNJziZ5NMk3k7xx9qNKktZjavCT7ACOAbcC+4EjSfavWvYIMF9VfwB8FfiHWQ8qSVqfIY/wbwKWqupcVT0H3AccmlxQVQ9U1bPjw1PAntmOKUlaryHB3w2cnzi+MD53KXcA37jYFUmOJllMsri8vDx8SknSus30RdsktwPzwL0Xu76qjlfVfFXNz83NzfKuJUlT7Byw5glg78TxnvG5/yfJ24EPA2+tql/OZjxJ0qwMeYR/GtiX5NokVwCHgYXJBUluAD4NHKyqJ2c/piRpvaYGv6qeB+4ETgKPAfdX1Zkk9yQ5OF52L/Bq4CtJ/j3JwiVuTpK0RYY8pUNVnQBOrDp398Tlt894LknSjPlOW0lqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpoYFPwkB5I8nmQpyV0Xuf43knx5fP1DSa6Z+aSSpHWZGvwkO4BjwK3AfuBIkv2rlt0BPF1Vvwv8E/DxWQ8qSVqfIY/wbwKWqupcVT0H3AccWrXmEPBv48tfBd6WJLMbU5K0XjsHrNkNnJ84vgD80aXWVNXzSZ4Bfhv42eSiJEeBo+PDXyb5/lqG3oZ2sWqvGnMvVrgXK9yLFb+31i8cEvyZqarjwHGAJItVNb+Z93+5ci9WuBcr3IsV7sWKJItr/dohT+k8AeydON4zPnfRNUl2Aq8FnlrrUJKk2RsS/NPAviTXJrkCOAwsrFqzAPzl+PJfAN+qqprdmJKk9Zr6lM74Ofk7gZPADuCzVXUmyT3AYlUtAP8KfCHJEvBzRj8Upjm+jrm3G/dihXuxwr1Y4V6sWPNexAfiktSD77SVpCYMviQ1seHB92MZVgzYiw8kOZvk0STfTPLGrZhzM0zbi4l170hSSbbtn+QN2Ysk7xx/b5xJ8sXNnnGzDPg/8oYkDyR5ZPz/5LatmHOjJflskicv9V6ljHxyvE+PJrlx0A1X1Yb9Y/Qi738AvwNcAXwP2L9qzV8BnxpfPgx8eSNn2qp/A/fiT4HfHF9+X+e9GK+7CngQOAXMb/XcW/h9sQ94BPit8fHrtnruLdyL48D7xpf3Az/a6rk3aC/+BLgR+P4lrr8N+AYQ4GbgoSG3u9GP8P1YhhVT96KqHqiqZ8eHpxi952E7GvJ9AfAxRp/L9IvNHG6TDdmL9wLHquppgKp6cpNn3CxD9qKA14wvvxb4ySbOt2mq6kFGf/F4KYeAz9fIKeDqJK+fdrsbHfyLfSzD7kutqarngRc+lmG7GbIXk+5g9BN8O5q6F+NfUfdW1dc3c7AtMOT74jrguiTfSXIqyYFNm25zDdmLjwK3J7kAnADevzmjXXZeak+ATf5oBQ2T5HZgHnjrVs+yFZK8AvgE8O4tHuVysZPR0zq3MPqt78Ekv19V/7WVQ22RI8Dnquofk/wxo/f/vLmq/merB3s52OhH+H4sw4ohe0GStwMfBg5W1S83abbNNm0vrgLeDHw7yY8YPUe5sE1fuB3yfXEBWKiqX1XVD4EfMPoBsN0M2Ys7gPsBquq7wKsYfbBaN4N6stpGB9+PZVgxdS+S3AB8mlHst+vztDBlL6rqmaraVVXXVNU1jF7POFhVa/7QqMvYkP8jX2P06J4kuxg9xXNuE2fcLEP24sfA2wCSvIlR8Jc3dcrLwwLwrvFf69wMPFNVP532RRv6lE5t3McyvOwM3It7gVcDXxm/bv3jqjq4ZUNvkIF70cLAvTgJ/HmSs8B/Ax+qqm33W/DAvfgg8Jkkf8voBdx3b8cHiEm+xOiH/K7x6xUfAV4JUFWfYvT6xW3AEvAs8J5Bt7sN90qSdBG+01aSmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElq4n8BzPZculjwdYoAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.subplot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NjOTtMjv63GY",
        "outputId": "9464a7bb-1f86-454b-ab5e-b861444fa0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.9.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.64.0)\n",
            "Collecting matplotlib>=3.4\n",
            "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 210 kB/s \n",
            "\u001b[?25hRequirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 966 kB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Collecting session-info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n",
            "Collecting anndata>=0.7.4\n",
            "  Downloading anndata-0.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.21.6)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (4.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=3->scanpy) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.8.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (1.4.2)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
            "\u001b[K     |████████████████████████████████| 930 kB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->scanpy) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: umap-learn, pynndescent, session-info\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=7646d70e51a5df7b55398df88249da67840206b3bb902a23a35184c4519efe47\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=988df47615307092975e1f6b4124d4c6e78afc391cf38735b3e1bdac96a29633\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8048 sha256=598a5c987099d470d7cd30e2bdb6b5bdb3dd979ac335f8c06ab08d91535a6a4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/ad/14/6a42359351a18337a8683854cfbba99dd782271f2d1767f87f\n",
            "Successfully built umap-learn pynndescent session-info\n",
            "Installing collected packages: fonttools, stdlib-list, pynndescent, matplotlib, umap-learn, session-info, anndata, scanpy\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed anndata-0.8.0 fonttools-4.33.3 matplotlib-3.5.2 pynndescent-0.5.6 scanpy-1.9.1 session-info-1.0.0 stdlib-list-0.8.0 umap-learn-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scvi-tools==0.8.1\n",
            "  Downloading scvi_tools-0.8.1-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (0.51.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (7.7.0)\n",
            "Collecting importlib-metadata<2.0,>=1.0\n",
            "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (1.0.2)\n",
            "Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (0.1.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (1.3.5)\n",
            "Requirement already satisfied: openpyxl>=3.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (1.21.6)\n",
            "Collecting rich>=6.2.0\n",
            "  Downloading rich-12.3.0-py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (4.64.0)\n",
            "Requirement already satisfied: anndata>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from scvi-tools==0.8.1) (0.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->scvi-tools==0.8.1) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->scvi-tools==0.8.1) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->scvi-tools==0.8.1) (4.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->scvi-tools==0.8.1) (2.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->scvi-tools==0.8.1) (1.15.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.5->scvi-tools==0.8.1) (21.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.5->scvi-tools==0.8.1) (4.2.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.5->scvi-tools==0.8.1) (5.5.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->scvi-tools==0.8.1) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0->scvi-tools==0.8.1) (3.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scvi-tools==0.8.1) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scvi-tools==0.8.1) (0.34.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0->scvi-tools==0.8.1) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20->anndata>=0.7.5->scvi-tools==0.8.1) (3.0.8)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->scvi-tools==0.8.1) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->scvi-tools==0.8.1) (2.8.2)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=6.2.0->scvi-tools==0.8.1) (2.6.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->scvi-tools==0.8.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->scvi-tools==0.8.1) (1.1.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools==0.8.1) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools==0.8.1) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools==0.8.1) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools==0.8.1) (5.1.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools==0.8.1) (5.3.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools==0.8.1) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->scvi-tools==0.8.1) (3.6.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools==0.8.1) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->scvi-tools==0.8.1) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools==0.8.1) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools==0.8.1) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools==0.8.1) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools==0.8.1) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->scvi-tools==0.8.1) (0.7.5)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->scvi-tools==0.8.1) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->scvi-tools==0.8.1) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->scvi-tools==0.8.1) (2.15.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->scvi-tools==0.8.1) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->scvi-tools==0.8.1) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->scvi-tools==0.8.1) (5.7.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets->scvi-tools==0.8.1) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (0.13.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (5.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->scvi-tools==0.8.1) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (2.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (5.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->scvi-tools==0.8.1) (0.5.1)\n",
            "Installing collected packages: importlib-metadata, commonmark, rich, scvi-tools\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed commonmark-0.9.1 importlib-metadata-1.7.0 rich-12.3.0 scvi-tools-0.8.1\n",
            "Collecting loompy\n",
            "  Downloading loompy-3.0.7.tar.gz (4.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from loompy) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from loompy) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from loompy) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from loompy) (57.4.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from loompy) (0.51.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from loompy) (7.1.2)\n",
            "Collecting numpy-groupies\n",
            "  Downloading numpy_groupies-0.9.15-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->loompy) (1.5.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->loompy) (0.34.0)\n",
            "Building wheels for collected packages: loompy\n",
            "  Building wheel for loompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for loompy: filename=loompy-3.0.7-py3-none-any.whl size=52040 sha256=36b4dc6ef52d21b0e7dd76cd23a5b5070b46bf86c1278234c089b38c3c13f96d\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/36/9f/eb3377d4a7423b96105b6667d36c3faa49ee73b96d8cab80b3\n",
            "Successfully built loompy\n",
            "Installing collected packages: numpy-groupies, loompy\n",
            "Successfully installed loompy-3.0.7 numpy-groupies-0.9.15\n",
            "Collecting leidenalg\n",
            "  Downloading leidenalg-0.8.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting igraph<0.10,>=0.9.0\n",
            "  Downloading igraph-0.9.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 36.4 MB/s \n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph, leidenalg\n",
            "Successfully installed igraph-0.9.10 leidenalg-0.8.10 texttable-1.6.4\n"
          ]
        }
      ],
      "source": [
        "%pip install scanpy\n",
        "%pip install scvi-tools==0.8.1\n",
        "%pip install loompy\n",
        "%pip install leidenalg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTE6OfMK9PKZ",
        "outputId": "54aa0ec7-be23-4694-d702-4dd8d010ebb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "memory_used = torch.cuda.memory_allocated()\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDkel2eaPXND",
        "outputId": "c057352d-5fdd-4313-a119-d65ed8fa1f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "joGjTBuACaeo"
      },
      "outputs": [],
      "source": [
        "# System\n",
        "import os, pathlib, time, gc\n",
        "\n",
        "# Math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Plots\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "import seaborn as sns\n",
        "\n",
        "# scvi\n",
        "import anndata\n",
        "import scvi\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8zcWXqLDsTg",
        "outputId": "768f6a88-3c3a-4dec-a35e-ccea3ceb62f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/scBIVI/scBIVI colab/scBIVI\n",
            "analysis.py\t download_data.sh     preprocess.py\trun_scBIVI.py\n",
            "bivae.py\t experiment_colab.py  __pycache__\trun.sh\n",
            "data\t\t gena_snippets\t      README.txt\tscBIVI.ipynb\n",
            "distribution.py  out\t\t      requirements.txt\tscBIVI.py\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/Shareddrives/scBIVI/scBIVI colab/scBIVI\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JsCFILc9CiWj"
      },
      "outputs": [],
      "source": [
        "# scbivi\n",
        "from scBIVI import scBIVI\n",
        "from analysis import calculate_accuracy, \\\n",
        "                     plot_corr_comparison, \\\n",
        "                     jaccard_index_split, \\\n",
        "                     knn_overlap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMklY4401inN"
      },
      "source": [
        "Manual scBIVI code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0lQ5lR7XUHd"
      },
      "source": [
        "# Load data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SHnXm3Pq6WCU"
      },
      "outputs": [],
      "source": [
        "outdir = 'out/pbmc_10k_v3'\n",
        "datadir = os.path.join(outdir,'data/preprocessed.h5ad')\n",
        "logdir = os.path.join(outdir,'out')\n",
        "percent_keep = 1\n",
        "cluster_method_split = 'RNA_leiden'\n",
        "\n",
        "## Load anndata\n",
        "dataext = pathlib.Path(datadir).suffix\n",
        "if dataext == '.h5ad':\n",
        "    adata = anndata.read_h5ad(datadir)\n",
        "elif dataext == '.loom':\n",
        "    adata = anndata.read_loom(datadir)\n",
        "\n",
        "## Downsample the data\n",
        "if percent_keep < 1:\n",
        "    X = adata.layers['counts']\n",
        "    # Convert to numpy array if not already\n",
        "    try:\n",
        "        X = X.toarray()\n",
        "    except:\n",
        "        pass\n",
        "    adata.layers['counts'] = np.random.binomial(X.astype('int32'),percent_keep)\n",
        "\n",
        "# Set up train/test data splits with 5-fold split\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "skf_splits = skf.split(adata, adata.obsm['Cluster'][cluster_method_split])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JjTA6uE4-MQ"
      },
      "source": [
        "# Run single instance for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw4p1LnWB6wf"
      },
      "source": [
        "Define hyper parameters and model (distribution) type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RbeH8gXNBvs-"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "lr       = 1e-3\n",
        "n_latent = 20 # overwritten by setup name\n",
        "n_epochs = 10\n",
        "# n_epochs = 20\n",
        "n_hidden = 1024\n",
        "n_layers = 2\n",
        "\n",
        "# model setup\n",
        "# scBIVI{dist}-{n_latent}-{data type}\n",
        "# type: \n",
        "setup = 'scBIVIcustom-10-combined'\n",
        "# setup = 'scBIVI-10-combined'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Oqc_DPEE4eM9"
      },
      "outputs": [],
      "source": [
        "# Use last of the K-fold splits\n",
        "for k, (train_index, test_index) in enumerate(skf_splits):\n",
        "  pass\n",
        "\n",
        "setups = [setup]\n",
        "\n",
        "cluster_methods = adata.obsm['Cluster'].columns.to_list()\n",
        "metrics_list = ['recon_error','latent embedding','compute'] + cluster_methods\n",
        "results_dict = {setup:{metrics: [] for metrics in metrics_list} for setup in setups}\n",
        "\n",
        "logdir_train = os.path.join(logdir,'train')\n",
        "os.makedirs(logdir_train, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NiVlRYSuid9v"
      },
      "outputs": [],
      "source": [
        "from torch._C import Value\n",
        "from seaborn.axisgrid import PairGrid\n",
        "# Define custom_dist if using scBIVIcustom\n",
        "\n",
        "\n",
        "# Set to default bivariate as example\n",
        "# from distribution import log_nb_positive_bi\n",
        "# custom_dist = log_nb_positive_bi\n",
        "def log_bursty_nb_bi(x: torch.Tensor, mu1: torch.Tensor, mu2: torch.Tensor,\n",
        "                       theta: torch.Tensor, eps=1e-8, **kwargs):\n",
        "    \"\"\"\n",
        "    Log likelihood (scalar) of a minibatch according to a bivariate nb model.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x\n",
        "        data\n",
        "    mu1,mu2\n",
        "        mean of the negative binomial (has to be positive support) (shape: minibatch x vars/2)\n",
        "    theta\n",
        "        params (has to be positive support) (shape: minibatch x vars)\n",
        "    eps\n",
        "        numerical stability constant\n",
        "    Notes\n",
        "    -----\n",
        "    We parametrize the bernoulli using the logits, hence the softplus functions appearing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Divide the original data x into spliced (x) and unspliced (y)\n",
        "    n,m = torch.chunk(x,2,dim=-1)\n",
        "\n",
        "    # THETA_IS_MAT_SHAPE = True #this breaks at 5%\n",
        "    # THETA_IS_B = False\n",
        "    # THETA_IS_NAS_SHAPE = False\n",
        "    \n",
        "    # THETA_IS_MAT_SHAPE = False #best performance: breaks at 25%\n",
        "    # THETA_IS_B = True\n",
        "    # THETA_IS_NAS_SHAPE = False\n",
        "    \n",
        "    THETA_IS_MAT_SHAPE = False #best performance: breaks at 65%\n",
        "    THETA_IS_B = False\n",
        "    THETA_IS_NAS_SHAPE = True\n",
        "    if THETA_IS_MAT_SHAPE:\n",
        "        gamma = 1/theta\n",
        "        b = mu2*gamma\n",
        "        beta = b/mu1\n",
        "    elif THETA_IS_B:\n",
        "        b = theta\n",
        "        beta = b/mu1\n",
        "        gamma = b/mu2\n",
        "    elif THETA_IS_NAS_SHAPE:\n",
        "        beta = 1/theta\n",
        "        b = mu1*beta\n",
        "        gamma = b/mu2\n",
        "\n",
        "\n",
        "    if torch.any(~torch.isfinite(gamma)):\n",
        "        filt = ~torch.isfinite(gamma)\n",
        "        print(gamma[filt])\n",
        "        print(b[filt])\n",
        "        print(beta[filt])\n",
        "        print(mu1[filt])\n",
        "        print(mu2[filt])\n",
        "        print(\"bad gamma\")\n",
        "        raise ValueError\n",
        "    if torch.any(~torch.isfinite(beta)):\n",
        "        print(\"bad beta\")\n",
        "        raise ValueError\n",
        "    if torch.any(~torch.isfinite(b)):\n",
        "        print(\"bad b\")\n",
        "        raise ValueError\n",
        "\n",
        "    var1 = mu1 * (1+b)\n",
        "    var2 = mu2 * (1+b*beta/(beta+gamma))\n",
        "    cov = b**2/(beta+gamma)\n",
        "    \n",
        "    logvar1 = torch.log((var1/mu1**2)+1)\n",
        "    logvar2 = torch.log((var2/mu2**2)+1)\n",
        "    logstd1 = torch.sqrt(logvar1)\n",
        "    logstd2 = torch.sqrt(logvar2)\n",
        "\n",
        "    logmean1 = torch.log(mu1**2/torch.sqrt(var1+mu1**2))\n",
        "    logmean2 = torch.log(mu2**2/torch.sqrt(var2+mu2**2))\n",
        "\n",
        "    if torch.any(~torch.isfinite(logmean1)):\n",
        "        print(\"bad logmean1\")\n",
        "        raise ValueError\n",
        "    if torch.any(~torch.isfinite(logmean2)):\n",
        "        print(\"bad logmean2\")\n",
        "        raise ValueError\n",
        "    if torch.any(~torch.isfinite(logstd1)):\n",
        "        print(\"bad logstd1\")\n",
        "        raise ValueError\n",
        "    if torch.any(~torch.isfinite(logstd2)):\n",
        "        print(\"bad logstd2\")\n",
        "        raise ValueError\n",
        "    if torch.any(~torch.isfinite(cov)):\n",
        "        print(\"bad cov\")\n",
        "        raise ValueError\n",
        "\n",
        "    logcov = torch.log(cov * torch.exp(-(logmean1 + logmean2 + (logvar1 + logvar2)/2)) +1 )\n",
        "    logcorr = logcov/torch.sqrt(logvar1 * logvar2)\n",
        "    if torch.any(torch.logical_or((logcorr<0), (logcorr>1))):\n",
        "        print('excuse me haha what')\n",
        "        filt = torch.logical_or((logcorr<0), (logcorr>1))\n",
        "        print('params')\n",
        "        # print(b[filt])/\n",
        "        print(gamma[filt][10])\n",
        "        print(beta[filt][10])\n",
        "        # print()\n",
        "        # print('logcov')\n",
        "        # print(logcov[filt])\n",
        "        # print('logcorr')\n",
        "        # print(logcorr[filt])\n",
        "        raise ValueError('Parameters are wrong!')\n",
        "\n",
        "    if torch.any(~torch.isfinite(logcov)):\n",
        "        raise ValueError(\"bad logcov\")\n",
        "    if torch.any(~torch.isfinite(logcorr)):\n",
        "        raise ValueError(\"bad logcorr\")\n",
        "\n",
        "    logmean_cond = logmean2 + logcorr * logstd2/logstd1 * (torch.log(n+1) - logmean1)\n",
        "    logvar_cond = logvar2 * (1-logcorr**2)  \n",
        "    # logstd_cond = logstd2 * torch.sqrt(1-logcorr**2)   \n",
        "    # logvar_cond = logstd_cond**2\n",
        "\n",
        "    if torch.any(~torch.isfinite(logmean_cond)):\n",
        "        raise ValueError(\"bad logmean_cond\")\n",
        "    if torch.any(~torch.isfinite(logcorr**2)):\n",
        "        raise ValueError('bad square')\n",
        "    if torch.any(~torch.isfinite((1-logcorr**2)**0.5)):\n",
        "        print('bad sqrt')\n",
        "        filt = ~torch.isfinite((1-logcorr**2)**0.5)\n",
        "        # print('logstd cond')\n",
        "        # print(logstd_cond[filt])\n",
        "        print('logstd2')\n",
        "        print(logstd2[filt])\n",
        "        print('logcorr')\n",
        "        print(logcorr[filt])\n",
        "        print('sqrt')\n",
        "        print(torch.sqrt(1-logcorr**2)   [filt])\n",
        "        print('radical argument')\n",
        "        print((1-logcorr**2)[filt])\n",
        "        raise ValueError('Something weird in sqrt')\n",
        "    if torch.any(~torch.isfinite(torch.sqrt(1-logcorr**2))):\n",
        "        raise ValueError('bad torch.sqrt')\n",
        "    # if torch.any(~torch.isfinite(logstd_cond)):\n",
        "    #     print(logstd2)\n",
        "    #     print(torch.sqrt(1-logcorr**2) )\n",
        "    #     raise ValueError(\"bad logstd_cond\")\n",
        "    if torch.any(~torch.isfinite(logvar_cond)):\n",
        "        raise ValueError(\"bad logvar_cond\")\n",
        "\n",
        "\n",
        "    mean_cond = torch.exp(logmean_cond + logvar_cond/2)\n",
        "    var_cond = torch.exp(2*logmean_cond + logvar_cond) * (torch.exp(logvar_cond) - 1)\n",
        "    if torch.any(~torch.isfinite(mean_cond)):\n",
        "        raise ValueError(\"bad mean_cond\")\n",
        "    if torch.any(~torch.isfinite(var_cond)):\n",
        "        raise ValueError(\"bad var_cond\")\n",
        "\n",
        "    r = 1/beta\n",
        "    r_cond = mean_cond**2/(var_cond-mean_cond)\n",
        "    p_cond = mean_cond/var_cond\n",
        "    prefactor = torch.lgamma(n+r) - torch.lgamma(n+1) - torch.lgamma(r) \\\n",
        "                + r * torch.log(r/(r+mu1)+eps) + n * torch.log(mu1/(r+mu1)+eps)\n",
        "\n",
        "    if torch.any(~torch.isfinite(r)):\n",
        "        raise ValueError(\"bad r\")\n",
        "    if torch.any(~torch.isfinite(prefactor)):\n",
        "        raise ValueError(\"bad prefactor\")\n",
        "\n",
        "\n",
        "    filt = torch.logical_and(torch.logical_and(r>0,p_cond>0), p_cond<1)\n",
        "\n",
        "    #compute the Poisson mean\n",
        "    y_ = m * torch.log(mean_cond+eps) - mean_cond - torch.lgamma(m+1) \n",
        "    y_[filt] += torch.lgamma(m[filt]+r_cond[filt]) - torch.lgamma(r_cond[filt]) \\\n",
        "                + r_cond[filt] * torch.log(r_cond[filt]/(r_cond[filt]+mean_cond[filt])+eps) \\\n",
        "                - m[filt] * torch.log(r_cond[filt]+mean_cond[filt]+eps) + mean_cond[filt]\n",
        "\n",
        "    # y_ = torch.lgamma(m+r_cond) - torch.lgamma(m+1) - torch.lgamma(r_cond) \\\n",
        "                # + r_cond * torch.log(r_cond/(r_cond+mean_cond)) \\\n",
        "                # + m * torch.log(mean_cond/(r_cond+mean_cond))\n",
        "    # y_ = torch.lgamma(m[filt]+r_cond[filt]) - torch.lgamma(m[filt]+1) - torch.lgamma(r_cond[filt]) \\\n",
        "    #             + r_cond[filt] * torch.log(r_cond[filt]/(r_cond[filt]+mean_cond[filt])) \\\n",
        "    #             + m[filt] * torch.log(mean_cond[filt]/(r_cond[filt]+mean_cond[filt]))\n",
        "    # y_[~filt] = m[~filt] * torch.log(mean_cond[~filt]) - mean_cond[~filt] - torch.lgamma(m[~filt]+1) \n",
        "    # print(y_.device)\n",
        "    # print(y_.shape)\n",
        "    # print(n.shape)\n",
        "    # print(filt.shape)\n",
        "    # print(y_.shape)\n",
        "    # print(prefactor.shape)\n",
        "    P = prefactor +  y_\n",
        "    if torch.any(~torch.isfinite(y_)):\n",
        "        raise ValueError('bad y_')\n",
        "    return P\n",
        "\n",
        "\n",
        "custom_dist = log_bursty_nb_bi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoP6NaxD6zPq",
        "outputId": "22fa2be0-c162-44e1-fe2f-6adb7c71fd36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "memory_used = torch.cuda.memory_allocated()\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.current_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "dYiLzRSA5DAq",
        "outputId": "c8eb2658-e3f9-4eb0-ec9b-b905a71b4639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34mINFO    \u001b[0m No batch_key inputted, assuming all cells are same batch                            \n",
            "\u001b[34mINFO    \u001b[0m No label_key inputted, assuming all cells have same label                           \n",
            "\u001b[34mINFO    \u001b[0m Using data from adata.layers\u001b[1m[\u001b[0m\u001b[32m\"counts\"\u001b[0m\u001b[1m]\u001b[0m                                              \n",
            "\u001b[34mINFO    \u001b[0m Computing library size prior per batch                                              \n",
            "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;36m11581\u001b[0m cells, \u001b[1;36m5428\u001b[0m vars, \u001b[1;36m1\u001b[0m batches,\n",
            "         \u001b[1;36m1\u001b[0m labels, and \u001b[1;36m0\u001b[0m proteins. Also registered \u001b[1;36m0\u001b[0m extra categorical covariates and \u001b[1;36m0\u001b[0m extra\n",
            "         continuous covariates.                                                              \n",
            "\u001b[34mINFO    \u001b[0m Please do not further modify adata until model is trained.                          \n",
            "scBIVIcustom\n",
            "\u001b[34mINFO    \u001b[0m Training for \u001b[1;36m10\u001b[0m epochs                                                              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at  ../aten/src/ATen/native/cuda/jit_utils.cpp:860.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34mINFO    \u001b[0m KL warmup phase exceeds overall training phaseIf your applications rely on the      \n",
            "         posterior quality, consider training for more epochs or reducing the kl warmup.     \n",
            "\u001b[34mINFO    \u001b[0m KL warmup for \u001b[1;36m400\u001b[0m epochs                                                            \n",
            "Training...:  70%|███████   | 7/10 [01:05<00:28,  9.34s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-33a7380b3b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mmetrics_to_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'reconstruction_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mfrequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             train_size = 0.9)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/models/vaemixin.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, train_size, test_size, lr, n_epochs_kl_warmup, n_iter_kl_warmup, frequency, train_fun_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training for {} epochs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_fun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_trained_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/trainers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, lr, eps, params, **extras_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_iteration_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# Update the model's parameters after seeing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Checks the training status, ensures no nan loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_iteration_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/trainers/trainer.py\u001b[0m in \u001b[0;36mon_training_loop\u001b[0;34m(self, tensors_dict)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/trainers/inference.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, tensors, feed_labels)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         reconst_loss, kl_divergence_local, kl_divergence_global = self.model(\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0msample_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_l_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_l_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m         loss = (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/modules/vae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, local_l_mean, local_l_var, batch_index, y)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \"\"\"\n\u001b[1;32m    426\u001b[0m         \u001b[0;31m# Parameters for z latent distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mqz_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qz_m\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mqz_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qz_v\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/modules/vae.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, x, batch_index, y, n_samples, transform_batch)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;31m# Sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mqz_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqz_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mql_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mql_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_observed_lib_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/modules/_base/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *cat_list)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mq_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mq_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreparameterize_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scvi/core/modules/_base/__init__.py\u001b[0m in \u001b[0;36mreparameterize_gaussian\u001b[0;34m(mu, var)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreparameterize_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 56\u001b[0;31m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34mf\"of distribution {repr(self)} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (128, 10)) of distribution Normal(loc: torch.Size([128, 10]), scale: torch.Size([128, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n       grad_fn=<AddmmBackward0>)"
          ]
        }
      ],
      "source": [
        "# print(setup)\n",
        "method,n_latent,datas = setup.split(\"-\")\n",
        "method_split = method.split('_')\n",
        "method = method_split[0]\n",
        "\n",
        "# If set up includes additional argument\n",
        "additional_kwargs = {s.split('=')[0]: float(s.split('=')[1]) for s in method_split[1:]}\n",
        "\n",
        "n_latent = int(n_latent)\n",
        "\n",
        "## Split the data\n",
        "if datas == 'spliced':\n",
        "  adata_in = adata[:,:int(adata.shape[1]/2)]\n",
        "elif datas == 'unspliced':\n",
        "  adata_in = adata[:,int(adata.shape[1]/2):]\n",
        "elif datas == 'combined':\n",
        "  adata_in = adata\n",
        "else:\n",
        "  raise ValueError(\"Input valid datas\")\n",
        "\n",
        "adata_in = adata_in.copy()\n",
        "scvi.data.setup_anndata(adata_in, layer=\"counts\")\n",
        "\n",
        "train_adata, test_adata = adata_in[train_index], adata_in[test_index]\n",
        "train_adata = train_adata.copy()\n",
        "\n",
        "## Set model parameters\n",
        "model_args = {'use_cuda'     : True,\n",
        "              'n_latent'     : n_latent,\n",
        "              'n_layers'     : n_layers,\n",
        "              'dispersion'   : 'gene',\n",
        "              'n_hidden'     : n_hidden,\n",
        "              'dropout_rate' :  0.1, #shouldn't use this at all\n",
        "              'gene_likelihood'    :  'nb',\n",
        "              'log_variational'    :  True,\n",
        "              'latent_distribution':  'normal'\n",
        "              }\n",
        "model_args.update(additional_kwargs)\n",
        "\n",
        "## Create model\n",
        "if method == 'LDVAE':\n",
        "    model = scvi.model.LinearSCVI(train_adata,**model_args)\n",
        "elif method == 'scVI':\n",
        "    model = scvi.model.SCVI(train_adata,**model_args)\n",
        "elif method == \"scBIVI\":\n",
        "    model = scBIVI(train_adata,mode='corr',**model_args)\n",
        "elif method == \"scBIVIuncorr\":\n",
        "    model = scBIVI(train_adata,mode='uncorr',**model_args)\n",
        "elif method == 'scBIVImixed':\n",
        "    model = scBIVI(train_adata,mode='mixed',**model_args)\n",
        "elif method == 'scBIVItemp':\n",
        "    model = scBIVI(train_adata,mode='mixed',**model_args)\n",
        "elif method == 'scBIVIcustom':\n",
        "    print(method)\n",
        "    model = scBIVI(train_adata,mode='custom',\n",
        "                   custom_dist=custom_dist,**model_args)\n",
        "else:\n",
        "    raise Exception('Input valid scVI model')\n",
        "\n",
        "## Train model\n",
        "start = time.time()\n",
        "model.train(n_epochs = n_epochs,\n",
        "            lr       = lr,\n",
        "            # n_epochs_kl_warmup = n_epochs/2,\n",
        "            metrics_to_monitor = ['reconstruction_error'],\n",
        "            frequency = 1,\n",
        "            train_size = 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRy6axjS-N7R"
      },
      "outputs": [],
      "source": [
        "runtime     = time.time() - start\n",
        "memory_used = torch.cuda.memory_allocated()\n",
        "results_dict[setup]['compute'].append([runtime,memory_used])\n",
        "\n",
        "## Check train history\n",
        "df_history = {'reconstruction_error_test_set' : model.history['reconstruction_error_test_set'],\n",
        "              'reconstruction_error_train_set': model.history['reconstruction_error_train_set']}\n",
        "df_history = pd.DataFrame(df_history)\n",
        "df_history = pd.DataFrame(df_history.stack())\n",
        "df = df_history\n",
        "df.reset_index(inplace=True)\n",
        "df.columns = ['Epoch','Loss Type', 'Loss']\n",
        "figname = f\"{setup}-{k}\"\n",
        "sns.lineplot(data=df, \n",
        "             x='Epoch', \n",
        "             y='Loss', \n",
        "             hue = 'Loss Type')\n",
        "# plt.savefig(os.path.join(logdir_train,f\"{figname}-train-history.pdf\"))\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPhWphSl-ap7"
      },
      "outputs": [],
      "source": [
        "## Get reconstruction loss on test data\n",
        "test_error  = model.get_reconstruction_error(test_adata)\n",
        "train_error = model.get_reconstruction_error(train_adata)\n",
        "results_dict[setup]['recon_error'].append(np.array([train_error,test_error]))\n",
        "\n",
        "## Extract the embedding space for scVI\n",
        "X_out = model.get_latent_representation(test_adata)\n",
        "\n",
        "adata_latent = anndata.AnnData(X_out)\n",
        "adata_latent.obs = test_adata.obs\n",
        "results_dict[setup]['latent embedding'] = adata_latent\n",
        "if datas == 'combined':\n",
        "  test_adata_save = test_adata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj-wH5kk5Pi0"
      },
      "outputs": [],
      "source": [
        "#### Validation with cluster accuracy based on labels\n",
        "## Iterate through ground truth labels based on different approach\n",
        "\n",
        "for cluster_method, y in test_adata.obsm['Cluster'].iteritems():\n",
        "\n",
        "    y = np.array(y.tolist())\n",
        "\n",
        "    score_dict = calculate_accuracy(X_out,y)\n",
        "    results_dict[setup][cluster_method].append(score_dict)\n",
        "\n",
        "#### Get predicted distribution\n",
        "if 'scBIVI' in method:\n",
        "\n",
        "    params = model.get_likelihood_parameters(test_adata)\n",
        "    mu = params['mean']\n",
        "    if method == 'scBIVImixed':\n",
        "        import torch.nn.functional as F\n",
        "        mu,mw = np.split(mu,2,axis=1)\n",
        "        mw_tensor = torch.tensor(mw.reshape(-1,int(mw.shape[1]/2),2))\n",
        "        mw_softmax = F.softmax(mw_tensor/10e-20,dim=-1)\n",
        "\n",
        "    mu1,mu2 = np.split(mu,2,axis=1)\n",
        "    alpha = params['dispersions']\n",
        "\n",
        "    if mu1.shape[1] == alpha.shape[1]:\n",
        "        stats = {'mu1': mu1, 'mu2' : mu2,\n",
        "                  '1/alpha'         : 1/alpha,\n",
        "                  'mu2/(mu1*alpha)' : mu2/(mu1*alpha),\n",
        "                  'mu2/alpha'       : mu2/alpha\n",
        "                  }\n",
        "\n",
        "        fig,axs = plt.subplots(1,len(stats),\n",
        "                                figsize=(5*len(stats),4),\n",
        "                                squeeze=False)\n",
        "\n",
        "        for ax, (stat,x) in zip(axs.reshape(-1),stats.items()):\n",
        "            ax = sns.histplot(np.log(x.reshape(-1)),kde=False,ax=ax,bins=20)\n",
        "            ax.set_title(stat)\n",
        "\n",
        "        # plt.savefig(os.path.join(logdir_train,f\"{figname}-hist.pdf\"))\n",
        "        # plt.close()\n",
        "\n",
        "#### Correlations\n",
        "\n",
        "# cg = plot_corr_comparison(X1,X2)\n",
        "# figname = f\"{setup}-{k}\"\n",
        "# plt.title(figname)\n",
        "# plt.savefig(os.path.join(logdir,f\"{figname}-corr.pdf\"))\n",
        "# plt.close()\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTfFQVxB_gKn"
      },
      "outputs": [],
      "source": [
        "#### Plot NLL\n",
        "setups = list(results_dict.keys())\n",
        "df_plot = pd.concat([pd.DataFrame({\"Train\": -np.array(r['recon_error'])[:,0],\n",
        "                                   \"Test\": -np.array(r['recon_error'])[:,1],\n",
        "                                   'Setup': key}) for key,r in results_dict.items()])\n",
        "\n",
        "df_plot['KFold'] = df_plot.index\n",
        "df_plot.reset_index(drop=True)\n",
        "\n",
        "df_plot.to_csv(os.path.join(logdir,'.svg'))\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "_ = sns.barplot(data=df_plot, x='Setup', y='Test', hue='Setup', dodge=False, ax=ax)\n",
        "ax.get_legend().remove()\n",
        "plt.xticks(rotation=45)\n",
        "# plt.savefig(os.path.join(logdir,'nll.svg'))\n",
        "# plt.close()\n",
        "\n",
        "print(df_plot.groupby(\"Setup\").mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGSWBFrL_k15"
      },
      "outputs": [],
      "source": [
        "#### Plot clustering accuracy\n",
        "\n",
        "# iterate through different cluster methods\n",
        "for cluster_method in cluster_methods:\n",
        "    df_plot = pd.concat([pd.DataFrame(r[cluster_method]).assign(Setup=key) for key,r in results_dict.items()])\n",
        "    df_plot.to_csv(os.path.join(logdir,f'clust_acc_{cluster_method}.csv'))\n",
        "    df_plot = df_plot.melt(id_vars=['Setup'],var_name='Metric',value_name='Score')\n",
        "\n",
        "    fig,ax=plt.subplots()\n",
        "    _ = sns.barplot(data=df_plot, x='Metric', y='Score', hue='Setup', ax=ax)\n",
        "    plt.xticks(rotation=45)\n",
        "    # plt.savefig(os.path.join(logdir,f'clust_acc_{cluster_method}.svg'))\n",
        "    # plt.close()\n",
        "    print(df_plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RWBNQNq4gPW"
      },
      "source": [
        "# Loop through all K-folds split and setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhUPtGZM-AM0"
      },
      "outputs": [],
      "source": [
        "# print(\"{}/{}\".format(torch.cuda.memory_allocated(),torch.cuda.max_memory_allocated()))\n",
        "\n",
        "# # Hyper-parameters\n",
        "# lr       = 1e-3\n",
        "# n_latent = 2\n",
        "# n_epochs = 20\n",
        "# n_hidden = 1024\n",
        "# n_layers = 3\n",
        "\n",
        "\n",
        "# # setups = ['scBIVI-10-combined',\n",
        "# #           'scVI-10-combined',\n",
        "# #           \"scVI-10-spliced\",\n",
        "# #           'scVI-10-unspliced',\n",
        "# #           'scBIVIuncorr-10-combined',\n",
        "# #           'scBIVImixed-10-combined']\n",
        "# # setups = ['scBIVItemp_T=0.000001_Trate=0-10-combined']\n",
        "# setups = ['scBIVIcustom-10-combined']\n",
        "\n",
        "# # setups = ['scBIVI-10-combined',\n",
        "# #           'scBIVItemp_T=1_Trate=0.01-10-combined',\n",
        "# #           'scBIVItemp_T=1_Trate=0.0001-10-combined',\n",
        "# #           'scBIVItemp_T=1_Trate=0.000001-10-combined',\n",
        "# #           'scBIVItemp_T=1_Trate=0.00000001-10-combined',\n",
        "# #           'scBIVItemp_T=1_Trate=0-10-combined',\n",
        "# #           'scBIVItemp_T=0.01_Trate=0-10-combined',\n",
        "# #           'scBIVItemp_T=0.0001_Trate=0-10-combined',\n",
        "# #           'scBIVItemp_T=0.000001_Trate=0-10-combined',]\n",
        "\n",
        "# cluster_methods = adata.obsm['Cluster'].columns.to_list()\n",
        "# metrics_list = ['recon_error','latent embedding','compute'] + cluster_methods\n",
        "# results_dict = {setup:{metrics: [] for metrics in metrics_list} for setup in setups}\n",
        "\n",
        "# logdir_train = os.path.join(logdir,'train')\n",
        "# os.makedirs(logdir_train, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoSUe5ih2f0F"
      },
      "outputs": [],
      "source": [
        "# for k, (train_index, test_index) in enumerate(skf_splits):\n",
        "#     #\n",
        "#   for setup in setups:\n",
        "#       #\n",
        "#     print(setup)\n",
        "#     method,n_latent,datas = setup.split(\"-\")\n",
        "#     method_split = method.split('_')\n",
        "#     method = method_split[0]\n",
        "#     additional_kwargs = {s.split('=')[0]:float(s.split('=')[1]) for s in method_split[1:]}\n",
        "\n",
        "#     n_latent = int(n_latent)\n",
        "\n",
        "#     ## Split the data\n",
        "#     if datas == 'spliced':\n",
        "#       adata_in = adata[:,:int(adata.shape[1]/2)]\n",
        "#     elif datas == 'unspliced':\n",
        "#       adata_in = adata[:,int(adata.shape[1]/2):]\n",
        "#     elif datas == 'combined':\n",
        "#       adata_in = adata\n",
        "#     else:\n",
        "#       raise ValueError(\"Input valid datas\")\n",
        "\n",
        "#     adata_in = adata_in.copy()\n",
        "#     scvi.data.setup_anndata(adata_in, layer=\"counts\")\n",
        "\n",
        "#     train_adata, test_adata = adata_in[train_index], adata_in[test_index]\n",
        "#     train_adata = train_adata.copy()\n",
        "\n",
        "#     ## Set model parameters\n",
        "#     model_args = {'use_cuda'     : True,\n",
        "#                   'n_latent'     : n_latent,\n",
        "#                   'n_layers'     : n_layers,\n",
        "#                   'dispersion'   : 'gene',\n",
        "#                   'n_hidden'     : n_hidden,\n",
        "#                   'dropout_rate' :  0.1,\n",
        "#                   'gene_likelihood'    :  'nb',\n",
        "#                   'log_variational'    :  True,\n",
        "#                   'latent_distribution':  'normal'\n",
        "#                   }\n",
        "#     model_args.update(additional_kwargs)\n",
        "\n",
        "#     ## Create model\n",
        "#     if method == 'LDVAE':\n",
        "#         model = scvi.model.LinearSCVI(train_adata,**model_args)\n",
        "#     elif method == 'scVI':\n",
        "#         model = scvi.model.SCVI(train_adata,**model_args)\n",
        "#     elif method == \"scBIVI\":\n",
        "#         model = scBIVI(train_adata,mode='corr',**model_args)\n",
        "#     elif method == \"scBIVIuncorr\":\n",
        "#         model = scBIVI(train_adata,mode='uncorr',**model_args)\n",
        "#     elif method == 'scBIVImixed':\n",
        "#         model = scBIVI(train_adata,mode='mixed',**model_args)\n",
        "#     elif method == 'scBIVItemp':\n",
        "#         # model_args['T'] = 1\n",
        "#         # model_args['Trate'] = 1\n",
        "#         model = scBIVI(train_adata,mode='mixed',**model_args)\n",
        "#     elif method == 'scBIVIcustom':\n",
        "#         model = scBIVI(train_adata,mode='custom',**model_args)\n",
        "#     else:\n",
        "#         raise Exception('Input valid scVI model')\n",
        "\n",
        "#     ## Train model\n",
        "#     start = time.time()\n",
        "#     model.train(n_epochs = n_epochs,\n",
        "#                 lr       = lr,\n",
        "#                 n_epochs_kl_warmup = n_epochs/2,\n",
        "#                 metrics_to_monitor = ['reconstruction_error'],\n",
        "#                 frequency = 1,\n",
        "#                 train_size = 0.9)\n",
        "\n",
        "#     runtime     = time.time() - start\n",
        "#     memory_used = torch.cuda.memory_allocated()\n",
        "#     results_dict[setup]['compute'].append([runtime,memory_used])\n",
        "\n",
        "#     ## Check train history\n",
        "#     df_history = {'reconstruction_error_test_set' : model.history['reconstruction_error_test_set'],\n",
        "#                   'reconstruction_error_train_set': model.history['reconstruction_error_train_set']}\n",
        "#     df_history = pd.DataFrame(df_history)\n",
        "#     df_history = pd.DataFrame(df_history.stack())\n",
        "#     df = df_history\n",
        "#     df.reset_index(inplace=True)\n",
        "#     df.columns = ['Epoch','Loss Type', 'Loss']\n",
        "#     figname = f\"{setup}-{k}\"\n",
        "#     sns.lineplot(data=df,x='Epoch', y='Loss', hue = 'Loss Type')\n",
        "#     plt.savefig(os.path.join(logdir_train,f\"{figname}-train-history.pdf\"))\n",
        "#     plt.close()\n",
        "\n",
        "#     ## Get reconstruction loss on test data\n",
        "#     test_error  = model.get_reconstruction_error(test_adata)\n",
        "#     train_error = model.get_reconstruction_error(train_adata)\n",
        "#     results_dict[setup]['recon_error'].append(np.array([train_error,test_error]))\n",
        "\n",
        "#     ## Extract the embedding space for scVI\n",
        "#     X_out = model.get_latent_representation(test_adata)\n",
        "\n",
        "#     if k == 0:\n",
        "#       adata_latent = anndata.AnnData(X_out)\n",
        "#       adata_latent.obs = test_adata.obs\n",
        "#       results_dict[setup]['latent embedding'] = adata_latent\n",
        "#       if datas == 'combined':\n",
        "#         test_adata_save = test_adata\n",
        "\n",
        "#     #### Validation with cluster accuracy based on labels\n",
        "#     ## Iterate through ground truth labels based on different approach\n",
        "\n",
        "#     for cluster_method, y in test_adata.obsm['Cluster'].iteritems():\n",
        "\n",
        "#         y = np.array(y.tolist())\n",
        "\n",
        "#         score_dict = calculate_accuracy(X_out,y)\n",
        "#         results_dict[setup][cluster_method].append(score_dict)\n",
        "\n",
        "#     #### Get predicted distribution\n",
        "#     if 'scBIVI' in method:\n",
        "\n",
        "#         params = model.get_likelihood_parameters(test_adata)\n",
        "#         mu = params['mean']\n",
        "#         if method == 'scBIVImixed':\n",
        "#             import torch.nn.functional as F\n",
        "#             mu,mw = np.split(mu,2,axis=1)\n",
        "#             mw_tensor = torch.tensor(mw.reshape(-1,int(mw.shape[1]/2),2))\n",
        "#             mw_softmax = F.softmax(mw_tensor/10e-20,dim=-1)\n",
        "\n",
        "#         mu1,mu2 = np.split(mu,2,axis=1)\n",
        "#         alpha = params['dispersions']\n",
        "\n",
        "#         if mu1.shape[1] == alpha.shape[1]:\n",
        "#             stats = {'mu1': mu1, 'mu2' : mu2,\n",
        "#                      '1/alpha'         : 1/alpha,\n",
        "#                      'mu2/(mu1*alpha)' : mu2/(mu1*alpha),\n",
        "#                      'mu2/alpha'       : mu2/alpha\n",
        "#                      }\n",
        "\n",
        "#             fig,axs = plt.subplots(1,len(stats),\n",
        "#                                    figsize=(5*len(stats),4),\n",
        "#                                    squeeze=False)\n",
        "\n",
        "#             for ax, (stat,x) in zip(axs.reshape(-1),stats.items()):\n",
        "#                 ax = sns.histplot(np.log(x.reshape(-1)),kde=False,ax=ax,bins=20)\n",
        "#                 ax.set_title(stat)\n",
        "\n",
        "#             plt.savefig(os.path.join(logdir_train,f\"{figname}-hist.pdf\"))\n",
        "#             plt.close()\n",
        "\n",
        "#     #### Correlations\n",
        "\n",
        "#     # cg = plot_corr_comparison(X1,X2)\n",
        "#     # figname = f\"{setup}-{k}\"\n",
        "#     # plt.title(figname)\n",
        "#     # plt.savefig(os.path.join(logdir,f\"{figname}-corr.pdf\"))\n",
        "#     # plt.close()\n",
        "\n",
        "#     del model\n",
        "#     torch.cuda.empty_cache()\n",
        "#     gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giCrK58d6mlg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "scBIVI.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}